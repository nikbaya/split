{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HailTable of height for UKB white British subset (used for GCTA rg estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Spark and Hail with default parameters...\n",
      "Running on Apache Spark version 2.2.3\n",
      "SparkUI available at http://10.128.0.37:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.11-5f8236706b33\n",
      "LOGGING: writing to /home/hail/hail-20190314-1746-0.2.11-5f8236706b33.log\n",
      "2019-03-14 17:46:54 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.\n",
      "    To preserve matrix table column order, first unkey columns with 'key_cols_by()'\n"
     ]
    }
   ],
   "source": [
    "gs_bucket = 'gs://nbaya/ldscsim/'\n",
    "mt0 = hl.read_matrix_table(gs_bucket+'hm3.50_sim_h2_0.08.mt/')\n",
    "ht0 = mt0.select_cols(mt0.nonsim_phen).cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht1 = ht0.rename({'s':'IID','nonsim_phen':'y'})\n",
    "ht1 = ht1.annotate(FID = '0')\n",
    "ht1 = ht1.key_by(ht1.FID)\n",
    "ht1 = ht1.select(ht1.IID,ht1.y)\n",
    "ht1 = ht1.key_by(ht1.IID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 17:21:44 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'f0' as type 'str' (type not specified)\n",
      "  Loading column 'f1' as type 'str' (type not specified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids = hl.import_table('gs://nbaya/split/gcta/gcta_20k.grm.id',no_header=True) #GRM ids\n",
    "ids = ids.rename({'f0':'FID','f1':'IID'})\n",
    "ids = set(ids.IID.take(ids.count()))\n",
    "ht2 = ht1.filter(hl.literal(ids).contains(ht1['IID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 17:22:26 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 17:22:28 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 17:22:30 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "| IID       |\n",
      "+-----------+\n",
      "| str       |\n",
      "+-----------+\n",
      "| \"1000844\" |\n",
      "| \"1001013\" |\n",
      "| \"1001278\" |\n",
      "| \"1001384\" |\n",
      "| \"1002023\" |\n",
      "| \"1002057\" |\n",
      "| \"1003314\" |\n",
      "| \"1003347\" |\n",
      "| \"1003529\" |\n",
      "| \"1003878\" |\n",
      "+-----------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ht2.IID.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 16:57:12 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 16:57:13 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 16:57:49 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 16:57:50 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 16:57:52 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+---------+\n",
      "| FID | IID       |        y1 |      y2 |\n",
      "+-----+-----------+-----------+---------+\n",
      "| str | str       |   float64 | float64 |\n",
      "+-----+-----------+-----------+---------+\n",
      "| \"0\" | \"1611860\" | -3.83e+00 |      NA |\n",
      "| \"0\" | \"3999820\" | -3.80e+00 |      NA |\n",
      "| \"0\" | \"2161718\" | -3.41e+00 |      NA |\n",
      "| \"0\" | \"5626326\" | -3.38e+00 |      NA |\n",
      "| \"0\" | \"2683239\" | -3.33e+00 |      NA |\n",
      "| \"0\" | \"2290186\" | -3.30e+00 |      NA |\n",
      "| \"0\" | \"2482129\" | -3.29e+00 |      NA |\n",
      "| \"0\" | \"3376535\" | -3.24e+00 |      NA |\n",
      "| \"0\" | \"1290594\" | -3.20e+00 |      NA |\n",
      "| \"0\" | \"2842188\" | -3.12e+00 |      NA |\n",
      "+-----+-----------+-----------+---------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 16:58:30 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 16:58:32 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 16:58:34 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+---------+\n",
      "| FID | IID       |        y1 |      y2 |\n",
      "+-----+-----------+-----------+---------+\n",
      "| str | str       |   float64 | float64 |\n",
      "+-----+-----------+-----------+---------+\n",
      "| \"0\" | \"3999820\" | -3.80e+00 |      NA |\n",
      "| \"0\" | \"5044464\" | -3.67e+00 |      NA |\n",
      "| \"0\" | \"2113870\" | -3.64e+00 |      NA |\n",
      "| \"0\" | \"1216856\" | -3.53e+00 |      NA |\n",
      "| \"0\" | \"5431973\" | -3.49e+00 |      NA |\n",
      "| \"0\" | \"5626326\" | -3.38e+00 |      NA |\n",
      "| \"0\" | \"2683239\" | -3.33e+00 |      NA |\n",
      "| \"0\" | \"5847550\" | -3.29e+00 |      NA |\n",
      "| \"0\" | \"2482129\" | -3.29e+00 |      NA |\n",
      "| \"0\" | \"1524138\" | -3.25e+00 |      NA |\n",
      "+-----+-----------+-----------+---------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 16:59:08 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 16:59:10 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 16:59:11 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+---------+\n",
      "| FID | IID       |        y1 |      y2 |\n",
      "+-----+-----------+-----------+---------+\n",
      "| str | str       |   float64 | float64 |\n",
      "+-----+-----------+-----------+---------+\n",
      "| \"0\" | \"1611860\" | -3.83e+00 |      NA |\n",
      "| \"0\" | \"1042750\" | -3.76e+00 |      NA |\n",
      "| \"0\" | \"2113870\" | -3.64e+00 |      NA |\n",
      "| \"0\" | \"5431973\" | -3.49e+00 |      NA |\n",
      "| \"0\" | \"2683239\" | -3.33e+00 |      NA |\n",
      "| \"0\" | \"2290186\" | -3.30e+00 |      NA |\n",
      "| \"0\" | \"5847550\" | -3.29e+00 |      NA |\n",
      "| \"0\" | \"1736350\" | -3.24e+00 |      NA |\n",
      "| \"0\" | \"3376535\" | -3.24e+00 |      NA |\n",
      "| \"0\" | \"3483084\" | -3.17e+00 |      NA |\n",
      "+-----+-----------+-----------+---------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 16:59:47 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 16:59:48 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 16:59:50 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+---------+\n",
      "| FID | IID       |        y1 |      y2 |\n",
      "+-----+-----------+-----------+---------+\n",
      "| str | str       |   float64 | float64 |\n",
      "+-----+-----------+-----------+---------+\n",
      "| \"0\" | \"1611860\" | -3.83e+00 |      NA |\n",
      "| \"0\" | \"1042750\" | -3.76e+00 |      NA |\n",
      "| \"0\" | \"2113870\" | -3.64e+00 |      NA |\n",
      "| \"0\" | \"1216856\" | -3.53e+00 |      NA |\n",
      "| \"0\" | \"1155634\" | -3.49e+00 |      NA |\n",
      "| \"0\" | \"5626326\" | -3.38e+00 |      NA |\n",
      "| \"0\" | \"2290186\" | -3.30e+00 |      NA |\n",
      "| \"0\" | \"5847550\" | -3.29e+00 |      NA |\n",
      "| \"0\" | \"2482129\" | -3.29e+00 |      NA |\n",
      "| \"0\" | \"1524138\" | -3.25e+00 |      NA |\n",
      "+-----+-----------+-----------+---------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 17:00:24 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 17:00:25 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 17:00:27 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+---------+\n",
      "| FID | IID       |        y1 |      y2 |\n",
      "+-----+-----------+-----------+---------+\n",
      "| str | str       |   float64 | float64 |\n",
      "+-----+-----------+-----------+---------+\n",
      "| \"0\" | \"1611860\" | -3.83e+00 |      NA |\n",
      "| \"0\" | \"3999820\" | -3.80e+00 |      NA |\n",
      "| \"0\" | \"5626326\" | -3.38e+00 |      NA |\n",
      "| \"0\" | \"2290186\" | -3.30e+00 |      NA |\n",
      "| \"0\" | \"2482129\" | -3.29e+00 |      NA |\n",
      "| \"0\" | \"1736350\" | -3.24e+00 |      NA |\n",
      "| \"0\" | \"3376535\" | -3.24e+00 |      NA |\n",
      "| \"0\" | \"5529174\" | -3.08e+00 |      NA |\n",
      "| \"0\" | \"5795174\" | -3.07e+00 |      NA |\n",
      "| \"0\" | \"2955572\" | -3.01e+00 |      NA |\n",
      "+-----+-----------+-----------+---------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c733ceef1af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#     ht.export(f'gs://nbaya/split/gcta/gcta_20k.s{i}.phen')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-922>\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, width, truncate, types, handler)\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/table.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, width, truncate, types, handler)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0mHandler\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \"\"\"\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'StructExpression'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/table.py\u001b[0m in \u001b[0;36m_show\u001b[0;34m(self, n, width, truncate, types)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhl_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m         \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0mhas_more\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-932>\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, n, _localize)\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/table.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, n, _localize)\u001b[0m\n\u001b[1;32m   1878\u001b[0m         \"\"\"\n\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_localize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtypecheck_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-926>\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, _localize)\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/table.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, _localize)\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_localize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/backend/backend.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, ir)\u001b[0m\n\u001b[1;32m     92\u001b[0m         return ir.typ._from_json(\n\u001b[1;32m     93\u001b[0m             Env.hail().backend.spark.SparkBackend.executeJSON(\n\u001b[0;32m---> 94\u001b[0;31m                 self._to_java_ir(ir)))\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = ht2.count()\n",
    "for i in range(1,11):\n",
    "    pi = [1]*int(n/2) + [0]*int(n/2)\n",
    "    randstate = np.random.RandomState(i)\n",
    "    randstate.shuffle(pi)\n",
    "    ht = ht2.add_index()\n",
    "    ht = ht.annotate(label = hl.literal(pi)[hl.int32(ht.idx)])\n",
    "    ht = ht.annotate(y1 = hl.cond(ht.label==1, ht.y, hl.null('float')))\n",
    "    ht = ht.annotate(y2 = hl.cond(ht.label==0, ht.y, hl.null('float')))\n",
    "    ht = ht.drop(ht.idx,ht.label,ht.y)\n",
    "    ht = ht.order_by(ht.y1)\n",
    "    ht.show()\n",
    "#     ht.export(f'gs://nbaya/split/gcta/gcta_20k.s{i}.phen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GWAS using same splits as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 17:47:33 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'f0' as type 'str' (type not specified)\n",
      "  Loading column 'f1' as type 'str' (type not specified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids = hl.import_table('gs://nbaya/split/gcta/gcta_20k.grm.id',no_header=True) #GRM ids\n",
    "ids = ids.rename({'f0':'FID','f1':'IID'})\n",
    "ids = set(ids.IID.take(ids.count()))\n",
    "mt1 = mt0.filter_cols(hl.literal(ids).contains(mt0.s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gwas(mt, x, y, cov_list=[], with_intercept=True, pass_through=[], path_to_save=None, \n",
    "         normalize_x=False, is_std_cov_list=False):\n",
    "    '''Runs GWAS'''\n",
    "    \n",
    "    mt = mt._annotate_all(col_exprs={'__y':y},\n",
    "                           entry_exprs={'__x':x})\n",
    "    if normalize_x:\n",
    "        mt = mt.annotate_rows(__gt_stats = hl.agg.stats(mt.__x))\n",
    "        mt = mt.annotate_entries(__x= (mt.__x-mt.__gt_stats.mean)/mt.__gt_stats.stdev) \n",
    "        mt = mt.drop('__gt_stats')\n",
    "    \n",
    "    if is_std_cov_list:\n",
    "        cov_list = ['isFemale','age','age_squared','age_isFemale',\n",
    "                    'age_squared_isFemale']+['PC{:}'.format(i) for i in range(1, 21)]\n",
    "        \n",
    "    if str in list(map(lambda x: type(x),cov_list)):\n",
    "        cov_list = list(map(lambda x: mt[x] if type(x) is str else x,cov_list))\n",
    "        \n",
    "    cov_list = ([1] if with_intercept else [])+cov_list\n",
    "    \n",
    "    pass_through = list(set(['rsid']+pass_through))\n",
    "    print(f'variables to pass through: {pass_through}')\n",
    "\n",
    "    gwas_ht = hl.linear_regression_rows(y=mt.__y,\n",
    "                                        x=mt.__x,\n",
    "                                        covariates=cov_list,\n",
    "                                        pass_through = pass_through)\n",
    "    \n",
    "    gwas_ht = gwas_ht.annotate_globals(with_intercept = with_intercept)\n",
    "        \n",
    "    gwas_ht = gwas_ht.rename({'rsid':'SNP'}).key_by('SNP')\n",
    "        \n",
    "    gwas_ht = gwas_ht.select(Z = gwas_ht.t_stat,\n",
    "                             N = gwas_ht.n)\n",
    "    \n",
    "    ss_template = hl.read_table('gs://nbaya/rg_sex/hm3.sumstats_template.ht') # sumstats template as a hail table\n",
    "    ss_template = ss_template.key_by('SNP')\n",
    "        \n",
    "    ss = ss_template.annotate(Z = gwas_ht[ss_template.SNP].Z,\n",
    "                              N = gwas_ht[ss_template.SNP].N)\n",
    "    \n",
    "    if path_to_save is not None:\n",
    "        ss.export(path_to_save)\n",
    "        \n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 17:57:02 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 17:57:03 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'FID' as type 'str' (type not specified)\n",
      "  Loading column 'IID' as type 'str' (type not specified)\n",
      "  Loading column 'y1' as type 'float64' (user-specified)\n",
      "  Loading column 'y2' as type 'float64' (user-specified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables to pass through: ['rsid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 17:57:45 Hail: WARN: 10000 of 20000 samples have a missing phenotype or covariate.\n",
      "2019-03-14 17:57:45 Hail: INFO: linear_regression_rows: running on 10000 samples for 1 response variable y,\n",
      "    with input variable x, and 26 additional covariates...\n",
      "2019-03-14 17:57:48 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 17:59:31 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 17:59:34 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:01:09 Hail: INFO: while writing:\n",
      "    gs://nbaya/split/gcta/20k_sumstats.y1.s1.tsv.bgz\n",
      "  merge time: 1.324s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables to pass through: ['rsid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 18:01:52 Hail: WARN: 10000 of 20000 samples have a missing phenotype or covariate.\n",
      "2019-03-14 18:01:52 Hail: INFO: linear_regression_rows: running on 10000 samples for 1 response variable y,\n",
      "    with input variable x, and 26 additional covariates...\n",
      "2019-03-14 18:01:55 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 18:03:37 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:03:37 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:05:05 Hail: INFO: while writing:\n",
      "    gs://nbaya/split/gcta/20k_sumstats.y2.s1.tsv.bgz\n",
      "  merge time: 1.037s\n",
      "2019-03-14 18:05:06 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'FID' as type 'str' (type not specified)\n",
      "  Loading column 'IID' as type 'str' (type not specified)\n",
      "  Loading column 'y1' as type 'float64' (user-specified)\n",
      "  Loading column 'y2' as type 'float64' (user-specified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables to pass through: ['rsid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 18:05:47 Hail: WARN: 10000 of 20000 samples have a missing phenotype or covariate.\n",
      "2019-03-14 18:05:47 Hail: INFO: linear_regression_rows: running on 10000 samples for 1 response variable y,\n",
      "    with input variable x, and 26 additional covariates...\n",
      "2019-03-14 18:05:49 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 18:07:24 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:07:27 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:09:04 Hail: INFO: while writing:\n",
      "    gs://nbaya/split/gcta/20k_sumstats.y1.s2.tsv.bgz\n",
      "  merge time: 1.211s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables to pass through: ['rsid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 18:09:49 Hail: WARN: 10000 of 20000 samples have a missing phenotype or covariate.\n",
      "2019-03-14 18:09:49 Hail: INFO: linear_regression_rows: running on 10000 samples for 1 response variable y,\n",
      "    with input variable x, and 26 additional covariates...\n",
      "2019-03-14 18:09:52 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 18:11:29 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:11:32 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:13:04 Hail: INFO: while writing:\n",
      "    gs://nbaya/split/gcta/20k_sumstats.y2.s2.tsv.bgz\n",
      "  merge time: 1.169s\n",
      "2019-03-14 18:13:04 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'FID' as type 'str' (type not specified)\n",
      "  Loading column 'IID' as type 'str' (type not specified)\n",
      "  Loading column 'y1' as type 'float64' (user-specified)\n",
      "  Loading column 'y2' as type 'float64' (user-specified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables to pass through: ['rsid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 18:13:46 Hail: WARN: 10000 of 20000 samples have a missing phenotype or covariate.\n",
      "2019-03-14 18:13:46 Hail: INFO: linear_regression_rows: running on 10000 samples for 1 response variable y,\n",
      "    with input variable x, and 26 additional covariates...\n",
      "2019-03-14 18:13:48 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 18:15:36 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:15:39 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:17:13 Hail: INFO: while writing:\n",
      "    gs://nbaya/split/gcta/20k_sumstats.y1.s3.tsv.bgz\n",
      "  merge time: 1.214s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables to pass through: ['rsid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 18:17:54 Hail: WARN: 10000 of 20000 samples have a missing phenotype or covariate.\n",
      "2019-03-14 18:17:54 Hail: INFO: linear_regression_rows: running on 10000 samples for 1 response variable y,\n",
      "    with input variable x, and 26 additional covariates...\n",
      "2019-03-14 18:17:56 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 18:19:51 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:19:53 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:21:40 Hail: INFO: while writing:\n",
      "    gs://nbaya/split/gcta/20k_sumstats.y2.s3.tsv.bgz\n",
      "  merge time: 1.027s\n",
      "2019-03-14 18:21:41 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'FID' as type 'str' (type not specified)\n",
      "  Loading column 'IID' as type 'str' (type not specified)\n",
      "  Loading column 'y1' as type 'float64' (user-specified)\n",
      "  Loading column 'y2' as type 'float64' (user-specified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables to pass through: ['rsid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 18:22:26 Hail: WARN: 10000 of 20000 samples have a missing phenotype or covariate.\n",
      "2019-03-14 18:22:26 Hail: INFO: linear_regression_rows: running on 10000 samples for 1 response variable y,\n",
      "    with input variable x, and 26 additional covariates...\n",
      "2019-03-14 18:22:28 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 18:24:09 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:24:09 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:25:40 Hail: INFO: while writing:\n",
      "    gs://nbaya/split/gcta/20k_sumstats.y1.s4.tsv.bgz\n",
      "  merge time: 1.262s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables to pass through: ['rsid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 18:26:21 Hail: WARN: 10000 of 20000 samples have a missing phenotype or covariate.\n",
      "2019-03-14 18:26:21 Hail: INFO: linear_regression_rows: running on 10000 samples for 1 response variable y,\n",
      "    with input variable x, and 26 additional covariates...\n",
      "2019-03-14 18:26:24 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-14 18:28:07 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-14 18:28:29 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "ename": "FatalError",
     "evalue": "SparkException: Job aborted due to stage failure: ResultStage 85 (saveAsTextFile at RichRDD.scala:66) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:513) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:444) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103) \tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat is.hail.sparkextras.ContextRDD.iterator(ContextRDD.scala:599) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:60) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:59) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$18.hasNext(Iterator.scala:764) \tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:331) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:327) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:216) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1374) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) \tat org.apache.spark.scheduler.Task.run(Task.scala:109) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \t... 1 more \n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 85 (saveAsTextFile at RichRDD.scala:66) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:513) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:444) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103) \tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat is.hail.sparkextras.ContextRDD.iterator(ContextRDD.scala:599) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:60) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:59) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$18.hasNext(Iterator.scala:764) \tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:331) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:327) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:216) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1374) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) \tat org.apache.spark.scheduler.Task.run(Task.scala:109) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1336)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1787)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1151)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1070)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1015)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:973)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:971)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:971)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:971)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply$mcV$sp(RDD.scala:1515)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1503)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1503)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1503)\n\tat is.hail.utils.richUtils.RichRDD$.writeTable$extension(RichRDD.scala:66)\n\tat is.hail.expr.ir.TableValue.export(TableValue.scala:148)\n\tat is.hail.expr.ir.Interpret$.apply(Interpret.scala:776)\n\tat is.hail.expr.ir.Interpret$.apply(Interpret.scala:86)\n\tat is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:31)\n\tat is.hail.backend.spark.SparkBackend$.execute(SparkBackend.scala:49)\n\tat is.hail.backend.spark.SparkBackend$.executeJSON(SparkBackend.scala:16)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\nHail version: 0.2.11-5f8236706b33\nError summary: SparkException: Job aborted due to stage failure: ResultStage 85 (saveAsTextFile at RichRDD.scala:66) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:513) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:444) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103) \tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat is.hail.sparkextras.ContextRDD.iterator(ContextRDD.scala:599) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:60) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:59) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$18.hasNext(Iterator.scala:764) \tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:331) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:327) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:216) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1374) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) \tat org.apache.spark.scheduler.Task.run(Task.scala:109) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \t... 1 more ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7c94203bd5fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                            y2 = phen[mt1.s].y2)\n\u001b[1;32m      6\u001b[0m     \u001b[0mgwas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdosage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_std_cov_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_to_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'gs://nbaya/split/gcta/20k_sumstats.y1.s{i}.tsv.bgz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgwas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdosage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_std_cov_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_to_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'gs://nbaya/split/gcta/20k_sumstats.y2.s{i}.tsv.bgz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-0c74906aa8a1>\u001b[0m in \u001b[0;36mgwas\u001b[0;34m(mt, x, y, cov_list, with_intercept, pass_through, path_to_save, normalize_x, is_std_cov_list)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpath_to_save\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-914>\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, output, types_file, header, parallel, delimiter)\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/table.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, output, types_file, header, parallel, delimiter)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         Env.backend().execute(\n\u001b[0;32m--> 997\u001b[0;31m             TableExport(self._tir, output, types_file, header, Env.hail().utils.ExportType.getExportType(parallel), delimiter))\n\u001b[0m\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnamed_exprs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'GroupedTable'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/backend/backend.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, ir)\u001b[0m\n\u001b[1;32m     92\u001b[0m         return ir.typ._from_json(\n\u001b[1;32m     93\u001b[0m             Env.hail().backend.spark.SparkBackend.executeJSON(\n\u001b[0;32m---> 94\u001b[0;31m                 self._to_java_ir(ir)))\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/utils/java.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n\u001b[1;32m    226\u001b[0m                              \u001b[0;34m'Hail version: %s\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                              'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n",
      "\u001b[0;31mFatalError\u001b[0m: SparkException: Job aborted due to stage failure: ResultStage 85 (saveAsTextFile at RichRDD.scala:66) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:513) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:444) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103) \tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat is.hail.sparkextras.ContextRDD.iterator(ContextRDD.scala:599) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:60) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:59) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$18.hasNext(Iterator.scala:764) \tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:331) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:327) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:216) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1374) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) \tat org.apache.spark.scheduler.Task.run(Task.scala:109) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \t... 1 more \n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 85 (saveAsTextFile at RichRDD.scala:66) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:513) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:444) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103) \tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat is.hail.sparkextras.ContextRDD.iterator(ContextRDD.scala:599) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:60) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:59) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$18.hasNext(Iterator.scala:764) \tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:331) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:327) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:216) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1374) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) \tat org.apache.spark.scheduler.Task.run(Task.scala:109) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1336)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1787)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1151)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1070)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1015)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:973)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:971)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:971)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:971)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply$mcV$sp(RDD.scala:1515)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1503)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1503)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1503)\n\tat is.hail.utils.richUtils.RichRDD$.writeTable$extension(RichRDD.scala:66)\n\tat is.hail.expr.ir.TableValue.export(TableValue.scala:148)\n\tat is.hail.expr.ir.Interpret$.apply(Interpret.scala:776)\n\tat is.hail.expr.ir.Interpret$.apply(Interpret.scala:86)\n\tat is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:31)\n\tat is.hail.backend.spark.SparkBackend$.execute(SparkBackend.scala:49)\n\tat is.hail.backend.spark.SparkBackend$.executeJSON(SparkBackend.scala:16)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\nHail version: 0.2.11-5f8236706b33\nError summary: SparkException: Job aborted due to stage failure: ResultStage 85 (saveAsTextFile at RichRDD.scala:66) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:513) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:444) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103) \tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat is.hail.sparkextras.ContextRDD.iterator(ContextRDD.scala:599) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:60) \tat is.hail.sparkextras.RepartitionedOrderedRDD2$$anonfun$compute$1$$anonfun$apply$1.apply(RepartitionedOrderedRDD2.scala:59) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$18.hasNext(Iterator.scala:764) \tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:331) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anonfun$20.apply(RowStore.scala:1709) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat is.hail.io.RichContextRDDRegionValue$$anonfun$boundary$extension$1$$anon$1.hasNext(RowStore.scala:1718) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004) \tat is.hail.utils.richUtils.RichIterator$$anon$5.isValid(RichIterator.scala:22) \tat is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:48) \tat is.hail.utils.FlipbookIterator$$anon$9.setValue(FlipbookIterator.scala:327) \tat is.hail.utils.FlipbookIterator$$anon$9.<init>(FlipbookIterator.scala:344) \tat is.hail.utils.FlipbookIterator.leftJoinDistinct(FlipbookIterator.scala:323) \tat is.hail.annotations.OrderedRVIterator.leftJoinDistinct(OrderedRVIterator.scala:62) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$6.apply(KeyedRVD.scala:88) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:98) \tat is.hail.rvd.KeyedRVD$$anonfun$orderedJoinDistinct$1.apply(KeyedRVD.scala:95) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$36.apply(ContextRDD.scala:469) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32$$anonfun$apply$33.apply(ContextRDD.scala:422) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438) \tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:216) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137) \tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1374) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145) \tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) \tat org.apache.spark.scheduler.Task.run(Task.scala:109) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId{streamId=397532422004, chunkIndex=0}: java.lang.RuntimeException: Executor is not registered (appId=application_1552581400009_0003, execId=53) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:171) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:105) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler$1.next(ExternalShuffleBlockHandler.java:95) \tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87) \tat org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat org.spark_project.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) \tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) \tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) \t... 1 more "
     ]
    }
   ],
   "source": [
    "n = mt1.count_cols()\n",
    "for i in range(1,101):\n",
    "    phen = hl.import_table(f'gs://nbaya/split/gcta/gcta_20k.s{i}.phen',types={'y1':hl.tfloat64,'y2':hl.tfloat64},key='IID')\n",
    "    mt = mt1.annotate_cols(y1 = phen[mt1.s].y1,\n",
    "                           y2 = phen[mt1.s].y2)\n",
    "    gwas(mt=mt,x=mt.dosage,y=mt.y1,is_std_cov_list=True,path_to_save=f'gs://nbaya/split/gcta/20k_sumstats.y1.s{i}.tsv.bgz')\n",
    "    gwas(mt=mt,x=mt.dosage,y=mt.y2,is_std_cov_list=True,path_to_save=f'gs://nbaya/split/gcta/20k_sumstats.y2.s{i}.tsv.bgz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hail",
   "language": "python",
   "name": "hail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}