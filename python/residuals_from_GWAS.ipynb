{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phen = '50_sim_inf'\n",
    "n_chunks = 300\n",
    "batch = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mt = hl.read_matrix_table('gs://nbaya/split/ukb31063.hm3_variants.gwas_samples_50_sim_inf_grouped300_batch_1_v2.mt/')\n",
    "mt = hl.read_matrix_table('gs://nbaya/split/meta_split/ukb31063.hm3_'+phen+'_gmt'+str(n_chunks)+'_batch_'+batch+'.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mt = hl.read_matrix_table('gs://nbaya/split/ukb31063.hm3_variants.gwas_samples_'+phen+'_grouped'+str(n_chunks)+'_batch_'+batch+'_v3.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(mt.beta.take(mt.count_rows()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.beta.take(mt.count_rows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.write('gs://nbaya/split/ukb31063.hm3_variants.gwas_samples_50_sim_inf_grouped300_batch_1_v3.mt',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mt.aggregate_cols(hl.agg.mean(mt.nonsim_phen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_set = 'qc_pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.read_matrix_table('gs://nbaya/split/ukb31063.'+variant_set+'_variants.gwas_samples_prerepart.mt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.describe())\n",
    "print(mt.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.count_rows())\n",
    "mt = mt.sample_rows(0.01)\n",
    "print(mt.count_rows())\n",
    "if variant_set == 'hm3':\n",
    "    mt = mt.repartition(1000)        \n",
    "elif variant_set == 'qc_pos':\n",
    "    mt = mt.repartition(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get sample of UKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "phen='50_raw'\n",
    "batch='1'\n",
    "n_chunks=300\n",
    "variant_set = 'hm3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6c4d28d93731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_matrix_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://nbaya/split/ukb31063.'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvariant_set\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_variants.gwas_samples_repart.mt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mphen\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'50_raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mphen_tb_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://nbaya/ukb31063.50_raw.tsv.bgz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimpute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtstr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mphen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'phen'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hl' is not defined"
     ]
    }
   ],
   "source": [
    "mt0 = hl.read_matrix_table('gs://nbaya/split/ukb31063.'+variant_set+'_variants.gwas_samples_repart.mt')\n",
    "\n",
    "if phen == '50_raw':\n",
    "    phen_tb_all = hl.import_table('gs://nbaya/ukb31063.50_raw.tsv.bgz',missing='',impute=True,types={'s': hl.tstr}).rename({phen: 'phen'})\n",
    "else:\n",
    "    phen_tb_all = hl.import_table('gs://phenotype_31063/ukb31063.phesant_phenotypes.both_sexes.tsv.bgz',\n",
    "                                  missing='',impute=True,types={'\"userId\"': hl.tstr}).rename({ '\"userId\"': 's', '\"'+phen+'\"': 'phen'})\n",
    "\n",
    "#Remove withdrawn samples\n",
    "withdrawn = hl.import_table('gs://nbaya/w31063_20181016.csv',missing='',no_header=True)\n",
    "withdrawn_set = set(withdrawn.f0.take(withdrawn.count()))\n",
    "phen_tb_all.describe()\n",
    "phen_tb_all = phen_tb_all.filter(hl.literal(withdrawn_set).contains(phen_tb_all['s']),keep=False) \n",
    "\n",
    "phen_tb = phen_tb_all.select(phen_tb_all['s'], phen_tb_all['phen'])\n",
    "phen_tb = phen_tb.key_by('s')\n",
    "phen_tb.count() #expect to be 361194 - 50 = 361144 for every phenotype before filtering, minus withdrawn samples \n",
    "\n",
    "mt1 = mt0.annotate_cols(phen_str = hl.str(phen_tb[mt0.s]['phen']).replace('\\\"',''))\n",
    "mt1 = mt1.filter_cols(mt1.phen_str == '',keep=False)\n",
    "\n",
    "if phen_tb.phen.dtype == hl.dtype('bool'):\n",
    "    mt1 = mt1.annotate_cols(phen = hl.bool(mt1.phen_str)).drop('phen_str')\n",
    "else:\n",
    "    mt1 = mt1.annotate_cols(phen = hl.float64(mt1.phen_str)).drop('phen_str')\n",
    "\n",
    "\n",
    "n_samples = mt1.count_cols()\n",
    "print('\\n>>> N samples = '+str(n_samples)+' <<<') #expect n samples to match n_non_missing from phenotypes.both_sexes.tsv\n",
    "mt2 = mt1.add_col_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = int(n_samples/n_chunks)+1     #the ideal number of samples in each group\n",
    "#list of group ids to be paired to each sample (Note: length of group_ids > # of cols in mt, but it doesn't affect the result)\n",
    "group_ids = np.ndarray.tolist(np.ndarray.flatten(np.asarray([range(n_chunks)]*group_size))) \n",
    "group_ids = group_ids[0:n_samples]\n",
    "randstate = np.random.RandomState(int(batch)) #seed with batch number\n",
    "randstate.shuffle(group_ids)\n",
    "mt3 = mt2.annotate_cols(group_id = hl.literal(group_ids)[hl.int32(mt2.col_idx)]) #assign group ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt4 = mt3.sample_rows(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt5 = mt4.filter_cols(mt4.group_id < 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-13 13:07:26 Hail: INFO: Coerced sorted dataset\n",
      "2018-12-13 13:13:00 Hail: INFO: wrote 11042 items in 1000 partitions to gs://nbaya/split/ukb31063.hm3_variants.gwas_samples_50.batch_1.downsample.mt\n"
     ]
    }
   ],
   "source": [
    "mt5.write('gs://nbaya/split/ukb31063.'+variant_set+'_variants.gwas_samples_'+phen+'.batch_'+batch+'.downsample_v2.mt') #Takes ~30 min with 50 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-13 13:05:03 Hail: INFO: Coerced sorted dataset\n",
      "2018-12-13 13:05:45 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11042, 6010)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://nbaya/split/ukb31063.hm3_variants.gwas_samples_50.batch_1.downsample.mt\n"
     ]
    }
   ],
   "source": [
    "print('gs://nbaya/split/ukb31063.'+variant_set+'_variants.gwas_samples_'+phen+'.batch_'+batch+'.downsample.mt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run conventional GWAS to obtain residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "phen='50_raw'\n",
    "batch='1'\n",
    "variant_set = 'hm3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Spark and Hail with default parameters...\n",
      "Running on Apache Spark version 2.2.1\n",
      "SparkUI available at http://10.128.0.11:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2-e60bdb1a125a\n",
      "LOGGING: writing to /home/hail/hail-20181214-2046-0.2-e60bdb1a125a.log\n",
      "2018-12-14 20:47:04 Hail: INFO: Reading table to impute column types\n",
      "2018-12-14 20:47:07 Hail: INFO: Finished type imputation\n",
      "  Loading column 's' as type 'str' (user-specified)\n",
      "  Loading column '50_raw' as type 'float64' (imputed)\n",
      "  Loading column 'phen_str' as type 'float64' (imputed)\n",
      "2018-12-14 20:47:07 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'f0' as type 'str' (type not specified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    's': str \n",
      "    'phen': float64 \n",
      "    'phen_str': float64 \n",
      "----------------------------------------\n",
      "Key: []\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-14 20:47:24 Hail: INFO: Coerced sorted dataset\n",
      "2018-12-14 20:47:37 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> N samples = 360338 <<<\n"
     ]
    }
   ],
   "source": [
    "mt0 = hl.read_matrix_table('gs://nbaya/split/ukb31063.'+variant_set+'_variants.gwas_samples_repart.mt')\n",
    "\n",
    "if phen == '50_raw':\n",
    "    phen_tb_all = hl.import_table('gs://nbaya/ukb31063.50_raw.tsv.bgz',missing='',impute=True,types={'s': hl.tstr}).rename({phen: 'phen'})\n",
    "else:\n",
    "    phen_tb_all = hl.import_table('gs://phenotype_31063/ukb31063.phesant_phenotypes.both_sexes.tsv.bgz',\n",
    "                                  missing='',impute=True,types={'\"userId\"': hl.tstr}).rename({ '\"userId\"': 's', '\"'+phen+'\"': 'phen'})\n",
    "\n",
    "#Remove withdrawn samples\n",
    "withdrawn = hl.import_table('gs://nbaya/w31063_20181016.csv',missing='',no_header=True)\n",
    "withdrawn_set = set(withdrawn.f0.take(withdrawn.count()))\n",
    "phen_tb_all.describe()\n",
    "phen_tb_all = phen_tb_all.filter(hl.literal(withdrawn_set).contains(phen_tb_all['s']),keep=False) \n",
    "\n",
    "phen_tb = phen_tb_all.select(phen_tb_all['s'], phen_tb_all['phen'])\n",
    "phen_tb = phen_tb.key_by('s')\n",
    "phen_tb.count() #expect to be 361194 - 50 = 361144 for every phenotype before filtering, minus withdrawn samples \n",
    "\n",
    "mt1 = mt0.annotate_cols(phen_str = hl.str(phen_tb[mt0.s]['phen']).replace('\\\"',''))\n",
    "mt1 = mt1.filter_cols(mt1.phen_str == '',keep=False)\n",
    "\n",
    "if phen_tb.phen.dtype == hl.dtype('bool'):\n",
    "    mt1 = mt1.annotate_cols(phen = hl.bool(mt1.phen_str)).drop('phen_str')\n",
    "else:\n",
    "    mt1 = mt1.annotate_cols(phen = hl.float64(mt1.phen_str)).drop('phen_str')\n",
    "\n",
    "\n",
    "n_samples = mt1.count_cols()\n",
    "print('\\n>>> N samples = '+str(n_samples)+' <<<') #expect n samples to match n_non_missing from phenotypes.both_sexes.tsv\n",
    "mt2 = mt1.add_col_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-14 20:51:38 Hail: INFO: Coerced sorted dataset\n"
     ]
    }
   ],
   "source": [
    "mt = mt1.rename({'dosage': 'x', 'phen': 'y'})\n",
    "    \n",
    "ht = mt.cols()\n",
    "    \n",
    "cov_list = [ ht['isFemale'], ht['age'], ht['age_squared'], ht['age_isFemale'],\n",
    "            ht['age_squared_isFemale'] ]+ [ht['PC{:}'.format(i)] for i in range(1, 21)] \n",
    "\n",
    "\n",
    "linreg_results = ht.aggregate(hl.agg.linreg(y=ht.y, x = [1] + cov_list))\n",
    "ht = ht.annotate(linreg = linreg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[179.59869007196897,\n",
       " -10.00499903973678,\n",
       " 0.038712916919730854,\n",
       " -0.0017666884720448197,\n",
       " -0.11712029809520526,\n",
       " 0.0010221997731952761,\n",
       " -42.78726612761782,\n",
       " 9.424978891676345,\n",
       " -43.46895320117373,\n",
       " 5.801306902992858,\n",
       " 6.904967719772118,\n",
       " -13.915209419018991,\n",
       " 20.632463911082937,\n",
       " 8.403526326706062,\n",
       " -17.477925981211122,\n",
       " -19.25762979048966,\n",
       " -9.517242479473383,\n",
       " -13.81377183530335,\n",
       " -11.955458535270052,\n",
       " -0.9854647801955304,\n",
       " 5.800489937988013,\n",
       " -21.47658234700417,\n",
       " -5.996207772991951,\n",
       " 1.1453617677615708,\n",
       " 15.555781939872313,\n",
       " -7.876648247655016]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_results.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = ht.annotate(y_pred =   linreg_results.beta[0]                +linreg_results.beta[1]*ht.isFemale    +linreg_results.beta[2]*ht.age+\n",
    "                            linreg_results.beta[3] *ht.age_squared+linreg_results.beta[4]*ht.age_isFemale+linreg_results.beta[5]*ht.age_squared_isFemale+\n",
    "                            linreg_results.beta[6] *ht.PC1        +linreg_results.beta[7]*ht.PC2         +linreg_results.beta[8]*ht.PC3+\n",
    "                            linreg_results.beta[9] *ht.PC4        +linreg_results.beta[10]*ht.PC5        +linreg_results.beta[11]*ht.PC6+\n",
    "                            linreg_results.beta[12]*ht.PC7        +linreg_results.beta[13]*ht.PC8        +linreg_results.beta[14]*ht.PC9+\n",
    "                            linreg_results.beta[15]*ht.PC10       +linreg_results.beta[16]*ht.PC11       +linreg_results.beta[17]*ht.PC12+\n",
    "                            linreg_results.beta[18]*ht.PC13       +linreg_results.beta[19]*ht.PC14       +linreg_results.beta[20]*ht.PC15+\n",
    "                            linreg_results.beta[21]*ht.PC16       +linreg_results.beta[22]*ht.PC17       +linreg_results.beta[23]*ht.PC18+\n",
    "                            linreg_results.beta[24]*ht.PC19       +linreg_results.beta[25]*ht.PC20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = ht.annotate(res = ht.y-ht.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-14 21:15:59 Hail: INFO: Coerced sorted dataset\n",
      "2018-12-14 21:16:11 Hail: INFO: wrote 360338 items in 4 partitions\n"
     ]
    }
   ],
   "source": [
    "ht.write('gs://nbaya/split/'+phen+'_linreg.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = mt_linreg.cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht.write('gs://nbaya/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FatalError",
     "evalue": "SparkException: Job 30 cancelled \n\nJava stack trace:\norg.apache.spark.SparkException: Job 30 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1704)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat is.hail.sparkextras.ContextRDD.treeAggregateWithPartitionOp(ContextRDD.scala:321)\n\tat is.hail.sparkextras.ContextRDD.treeAggregateWithPartitionOp(ContextRDD.scala:281)\n\tat is.hail.rvd.RVD.treeAggregateWithPartitionOp(RVD.scala:589)\n\tat is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1733)\n\tat is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1830)\n\tat is.hail.expr.ir.MatrixColsTable.execute(TableIR.scala:781)\n\tat is.hail.expr.ir.TableHead.execute(TableIR.scala:318)\n\tat is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:653)\n\tat is.hail.expr.ir.TableMapRows.execute(TableIR.scala:511)\n\tat is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:653)\n\tat is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:653)\n\tat is.hail.expr.ir.TableMapRows.execute(TableIR.scala:511)\n\tat is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:653)\n\tat is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:653)\n\tat is.hail.table.Table.value$lzycompute(Table.scala:202)\n\tat is.hail.table.Table.value(Table.scala:200)\n\tat is.hail.table.Table.rdd$lzycompute(Table.scala:209)\n\tat is.hail.table.Table.rdd(Table.scala:209)\n\tat is.hail.table.Table.collect(Table.scala:540)\n\tat is.hail.table.Table.showString(Table.scala:597)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\nHail version: 0.2-e60bdb1a125a\nError summary: SparkException: Job 30 cancelled ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b6344c8c93fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misFemale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-420>\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, width, truncate, types, handler)\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/expr/expressions/base_expression.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, width, truncate, types, handler)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mPrint\u001b[0m \u001b[0man\u001b[0m \u001b[0mextra\u001b[0m \u001b[0mheader\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \"\"\"\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/expr/expressions/base_expression.py\u001b[0m in \u001b[0;36m_show\u001b[0;34m(self, n, width, truncate, types)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/table.py\u001b[0m in \u001b[0;36m_show\u001b[0;34m(self, n, width, truncate, types)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/utils/java.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n\u001b[1;32m    209\u001b[0m                              \u001b[0;34m'Hail version: %s\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                              'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n",
      "\u001b[0;31mFatalError\u001b[0m: SparkException: Job 30 cancelled \n\nJava stack trace:\norg.apache.spark.SparkException: Job 30 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1704)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat is.hail.sparkextras.ContextRDD.treeAggregateWithPartitionOp(ContextRDD.scala:321)\n\tat is.hail.sparkextras.ContextRDD.treeAggregateWithPartitionOp(ContextRDD.scala:281)\n\tat is.hail.rvd.RVD.treeAggregateWithPartitionOp(RVD.scala:589)\n\tat is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1733)\n\tat is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1830)\n\tat is.hail.expr.ir.MatrixColsTable.execute(TableIR.scala:781)\n\tat is.hail.expr.ir.TableHead.execute(TableIR.scala:318)\n\tat is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:653)\n\tat is.hail.expr.ir.TableMapRows.execute(TableIR.scala:511)\n\tat is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:653)\n\tat is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:653)\n\tat is.hail.expr.ir.TableMapRows.execute(TableIR.scala:511)\n\tat is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:653)\n\tat is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:653)\n\tat is.hail.table.Table.value$lzycompute(Table.scala:202)\n\tat is.hail.table.Table.value(Table.scala:200)\n\tat is.hail.table.Table.rdd$lzycompute(Table.scala:209)\n\tat is.hail.table.Table.rdd(Table.scala:209)\n\tat is.hail.table.Table.collect(Table.scala:540)\n\tat is.hail.table.Table.showString(Table.scala:597)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\nHail version: 0.2-e60bdb1a125a\nError summary: SparkException: Job 30 cancelled "
     ]
    }
   ],
   "source": [
    "ht.annotate(y_pred = ht.linreg.beta[1]*ht.isFemale).y_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add residues to phen_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-14 21:27:03 Hail: INFO: Reading table to impute column types\n",
      "2018-12-14 21:27:04 Hail: INFO: Finished type imputation\n",
      "  Loading column 's' as type 'str' (user-specified)\n",
      "  Loading column '50_raw' as type 'float64' (imputed)\n",
      "  Loading column 'phen_str' as type 'float64' (imputed)\n"
     ]
    }
   ],
   "source": [
    "phen = '50_raw_res'\n",
    "ht = hl.read_table('gs://nbaya/split/50_raw_linreg.ht')\n",
    "phen_tb_all = hl.import_table('gs://nbaya/ukb31063.50_raw.tsv.bgz',missing='',impute=True,types={'s': hl.tstr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    's': str \n",
      "    '50_raw': float64 \n",
      "    'phen_str': float64 \n",
      "----------------------------------------\n",
      "Key: []\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "phen_tb_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    's': str \n",
      "    'isFemale': bool \n",
      "    'age': int32 \n",
      "    'age_squared': int32 \n",
      "    'age_isFemale': int32 \n",
      "    'age_squared_isFemale': int32 \n",
      "    'PC1': float64 \n",
      "    'PC2': float64 \n",
      "    'PC3': float64 \n",
      "    'PC4': float64 \n",
      "    'PC5': float64 \n",
      "    'PC6': float64 \n",
      "    'PC7': float64 \n",
      "    'PC8': float64 \n",
      "    'PC9': float64 \n",
      "    'PC10': float64 \n",
      "    'PC11': float64 \n",
      "    'PC12': float64 \n",
      "    'PC13': float64 \n",
      "    'PC14': float64 \n",
      "    'PC15': float64 \n",
      "    'PC16': float64 \n",
      "    'PC17': float64 \n",
      "    'PC18': float64 \n",
      "    'PC19': float64 \n",
      "    'PC20': float64 \n",
      "    'y': float64 \n",
      "    'linreg': struct {\n",
      "        beta: array<float64>, \n",
      "        standard_error: array<float64>, \n",
      "        t_stat: array<float64>, \n",
      "        p_value: array<float64>, \n",
      "        multiple_standard_error: float64, \n",
      "        multiple_r_squared: float64, \n",
      "        adjusted_r_squared: float64, \n",
      "        f_stat: float64, \n",
      "        multiple_p_value: float64, \n",
      "        n: int32\n",
      "    } \n",
      "    'y_pred': float64 \n",
      "    'phen': float64 \n",
      "----------------------------------------\n",
      "Key: ['s']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hl.read_table('gs://nbaya/split/50_raw_linreg.ht').rename({'res': 'phen'}).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hail",
   "language": "python",
   "name": "hail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}